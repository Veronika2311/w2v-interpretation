{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция для первой домашки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. взять лучший теггер для русского языка и с его помощью написать функцию, которая повысит качество работы программы из первой домашки. Так, многие из вас справедливо заметили, что если бы мы могли класть в словарь не только отдельные слова, но и словосочетания, то программа работала бы лучше. Вам надо выделить 3 вида синтаксических групп (к примеру не + какая-то часть речи или NP или сущ.+ наречие или еще что-то), запись которых в словарь, по вашему мнению, улучшила бы качество работы программы и создать такую функцию или функции, которые с помощью любых известных нам средств (chunking и regexp grammar, Natasha syntax parser, код с последнего занятия по SpyCy, etc.) будет выделять эти группы в поданном в нее тексте. Два балла за саму функцию, балл за объяснение того, почему именно эти группы вы взяли\n",
    "5. Встроить эту функцию в программу из предыдущей домашки и сравните качества работы программы с нею и без неё 2 бонусных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) В принципе, мы можем предположить, что не в вершине группы прилагательного должно заставлять нас добавлять балл противоположной тональности.\n",
    "2) Кроме того, глагол + наречие хорошо бы рассматривать вместе.\n",
    "3) В-третьих, можно попробовать рассматривать вместе существительные и глаголы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании, конечно, сказано \"взять лучший теггер\", но. Почему здесь крайне неудачная идея использовать майстем? Он медленный. Если мы не выделяем дополнительных файлов на запись/чтение и хотели бы получить результат через более-менее адекватное время, то лучше бы мы его не брали, так как 500+ довольно длинных отзывов он будет обрабатывать очень долго. Это вряд ли именно то, что мы бы хотели. Поэтому мы можем немного пожертвовать точностью, но зато получить результаты более быстро."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот функция из прошлой домашки. И еслии мы хотим получить то, что нам нужно, не за такое время, то поменять нужно крайне немного. К тому же, здесь уже используется лемматизация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(hrefs):\n",
    "    words = []\n",
    "    for href in hrefs: #tqdm(hrefs):\n",
    "        href = f'https://bookmix.ru/{href}'\n",
    "        req = session.get(href, headers={'User-Agent': ua.random})\n",
    "        req.encoding = 'utf-8'\n",
    "        req = req.text\n",
    "        req = BeautifulSoup(req, 'html.parser')\n",
    "        text = req.find('div', {'class': 'universal-blocks-content'}).text.lower()\n",
    "        \n",
    "        text = word_tokenize(text) #вот здесь нужен разбор\n",
    "        for word in text:\n",
    "            if word.isalpha():\n",
    "                token = morph.parse(word)\n",
    "                words.append(morph.parse(word)[0][2] + '_' + ) #добаляем в словарь лемму\n",
    "    words = dict(Counter(words))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='кошка', tag=OpencorporaTag('NOUN,anim,femn sing,nomn'), normal_form='кошка', score=0.833333, methods_stack=((<DictionaryAnalyzer>, 'кошка', 132, 0),)),\n",
       " Parse(word='кошка', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='кошка', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'кошка', 9, 0),))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('кошка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tone(href):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    words = get_texts([href,])\n",
    "    for word, freq in words.items():\n",
    "        if word in positive_set:\n",
    "            pos += 1*freq\n",
    "        elif word in negative_set:\n",
    "            neg += 1*freq\n",
    "    if pos > neg:\n",
    "        result = 'pos'\n",
    "    elif pos < neg:\n",
    "        result = 'neg'\n",
    "    else:\n",
    "        result = 'nonloso'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synt_grams(text):\n",
    "    full_stem = m.analyze(TEXT1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_result = []\n",
    "\n",
    "for word in full_stem:\n",
    "    if word.get('analysis') != None:\n",
    "        mystem_dict = {}\n",
    "        mystem_dict['слово'] = word['text'].lower()\n",
    "        mystem_dict['разбор'] = convert_tags(word['analysis'][0]['gr'].split('=')[0].split(',')[0])\n",
    "        mystem_result.append(mystem_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 30, 5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([1, 2, 3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
